{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bfa6b55-8fba-4e08-88ea-e69743c6cd87",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Nitya\\Documents\\Zoom\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 2.7725892066955566\n",
      "Epoch 0, Loss: 2.7725892066955566\n",
      "Epoch 0, Loss: 2.7725892066955566\n",
      "Epoch 0, Loss: 2.7725892066955566\n",
      "Epoch 0, Loss: 2.7725892066955566\n",
      "Epoch 0, Loss: 2.7725892066955566\n",
      "Epoch 0, Loss: 2.7725892066955566\n",
      "Epoch 0, Loss: 2.7725892066955566\n",
      "Epoch 0, Loss: 2.7725892066955566\n",
      "Epoch 0, Loss: 2.7725892066955566\n",
      "Epoch 0, Loss: 2.7725892066955566\n",
      "Epoch 0, Loss: 2.7725892066955566\n",
      "Epoch 0, Loss: 2.7725892066955566\n",
      "Epoch 0, Loss: 2.7725892066955566\n",
      "Epoch 0, Loss: 2.7725892066955566\n",
      "Epoch 0, Loss: 2.7725892066955566\n",
      "Epoch 0, Loss: 2.7725892066955566\n",
      "Epoch 0, Loss: 2.7725892066955566\n",
      "Epoch 0, Loss: 2.7725892066955566\n",
      "Epoch 0, Loss: 2.7725892066955566\n",
      "Epoch 0, Loss: 2.7725892066955566\n",
      "Epoch 0, Loss: 2.7725892066955566\n",
      "Epoch 0, Loss: 2.7725892066955566\n",
      "Epoch 0, Loss: 2.7725892066955566\n",
      "Epoch 0, Loss: 2.7725892066955566\n",
      "Epoch 0, Loss: 2.7725892066955566\n",
      "Epoch 0, Loss: 2.7725892066955566\n",
      "Epoch 0, Loss: 2.7725892066955566\n",
      "Epoch 0, Loss: 2.7725892066955566\n",
      "Epoch 0, Loss: 2.7725892066955566\n",
      "Epoch 0, Loss: 2.7725892066955566\n",
      "Epoch 0, Loss: 2.7725892066955566\n",
      "Epoch 0, Loss: 2.7725892066955566\n",
      "Epoch 0, Loss: 2.7725892066955566\n",
      "Epoch 0, Loss: 2.7725892066955566\n",
      "Epoch 0, Loss: 2.7725892066955566\n",
      "Epoch 0, Loss: 2.7725892066955566\n",
      "Epoch 0, Loss: 2.7725892066955566\n",
      "Epoch 0, Loss: 2.7725892066955566\n",
      "Epoch 0, Loss: 2.7725892066955566\n",
      "Epoch 0, Loss: 2.7725892066955566\n",
      "Epoch 0, Loss: 2.7725892066955566\n",
      "Epoch 0, Loss: 2.7725892066955566\n",
      "Epoch 0, Loss: 2.7725892066955566\n",
      "Epoch 0, Loss: 2.7725892066955566\n",
      "Epoch 0, Loss: 2.7725892066955566\n",
      "Epoch 0, Loss: 2.7725892066955566\n",
      "Epoch 0, Loss: 2.7725892066955566\n",
      "Epoch 0, Loss: 2.7725892066955566\n",
      "Epoch 0, Loss: 2.7725892066955566\n",
      "Epoch 0, Loss: 2.7725892066955566\n",
      "Epoch 0, Loss: 1.3862943649291992\n",
      "Epoch 1, Loss: 2.7725892066955566\n",
      "Epoch 1, Loss: 2.7725892066955566\n",
      "Epoch 1, Loss: 2.7725892066955566\n",
      "Epoch 1, Loss: 2.7725892066955566\n",
      "Epoch 1, Loss: 2.7725892066955566\n",
      "Epoch 1, Loss: 2.7725892066955566\n",
      "Epoch 1, Loss: 2.7725892066955566\n",
      "Epoch 1, Loss: 2.7725892066955566\n",
      "Epoch 1, Loss: 2.7725892066955566\n",
      "Epoch 1, Loss: 2.7725892066955566\n",
      "Epoch 1, Loss: 2.7725892066955566\n",
      "Epoch 1, Loss: 2.7725892066955566\n",
      "Epoch 1, Loss: 2.7725892066955566\n",
      "Epoch 1, Loss: 2.7725892066955566\n",
      "Epoch 1, Loss: 2.7725892066955566\n",
      "Epoch 1, Loss: 2.7725892066955566\n",
      "Epoch 1, Loss: 2.7725892066955566\n",
      "Epoch 1, Loss: 2.7725892066955566\n",
      "Epoch 1, Loss: 2.7725892066955566\n",
      "Epoch 1, Loss: 2.7725892066955566\n",
      "Epoch 1, Loss: 2.7725892066955566\n",
      "Epoch 1, Loss: 2.7725892066955566\n",
      "Epoch 1, Loss: 2.7725892066955566\n",
      "Epoch 1, Loss: 2.7725892066955566\n",
      "Epoch 1, Loss: 2.7725892066955566\n",
      "Epoch 1, Loss: 2.7725892066955566\n",
      "Epoch 1, Loss: 2.7725892066955566\n",
      "Epoch 1, Loss: 2.7725892066955566\n",
      "Epoch 1, Loss: 2.7725892066955566\n",
      "Epoch 1, Loss: 2.7725892066955566\n",
      "Epoch 1, Loss: 2.7725892066955566\n",
      "Epoch 1, Loss: 2.7725892066955566\n",
      "Epoch 1, Loss: 2.7725892066955566\n",
      "Epoch 1, Loss: 2.7725892066955566\n",
      "Epoch 1, Loss: 2.7725892066955566\n",
      "Epoch 1, Loss: 2.7725892066955566\n",
      "Epoch 1, Loss: 2.7725892066955566\n",
      "Epoch 1, Loss: 2.7725892066955566\n",
      "Epoch 1, Loss: 2.7725892066955566\n",
      "Epoch 1, Loss: 2.7725892066955566\n",
      "Epoch 1, Loss: 2.7725892066955566\n",
      "Epoch 1, Loss: 2.7725892066955566\n",
      "Epoch 1, Loss: 2.7725892066955566\n",
      "Epoch 1, Loss: 2.7725892066955566\n",
      "Epoch 1, Loss: 2.7725892066955566\n",
      "Epoch 1, Loss: 2.7725892066955566\n",
      "Epoch 1, Loss: 2.7725892066955566\n",
      "Epoch 1, Loss: 2.7725892066955566\n",
      "Epoch 1, Loss: 2.7725892066955566\n",
      "Epoch 1, Loss: 2.7725892066955566\n",
      "Epoch 1, Loss: 2.7725892066955566\n",
      "Epoch 1, Loss: 1.3862943649291992\n",
      "Epoch 2, Loss: 2.7725892066955566\n",
      "Epoch 2, Loss: 2.7725892066955566\n",
      "Epoch 2, Loss: 2.7725892066955566\n",
      "Epoch 2, Loss: 2.7725892066955566\n",
      "Epoch 2, Loss: 2.7725892066955566\n",
      "Epoch 2, Loss: 2.7725892066955566\n",
      "Epoch 2, Loss: 2.7725892066955566\n",
      "Epoch 2, Loss: 2.7725892066955566\n",
      "Epoch 2, Loss: 2.7725892066955566\n",
      "Epoch 2, Loss: 2.7725892066955566\n",
      "Epoch 2, Loss: 2.7725892066955566\n",
      "Epoch 2, Loss: 2.7725892066955566\n",
      "Epoch 2, Loss: 2.7725892066955566\n",
      "Epoch 2, Loss: 2.7725892066955566\n",
      "Epoch 2, Loss: 2.7725892066955566\n",
      "Epoch 2, Loss: 2.7725892066955566\n",
      "Epoch 2, Loss: 2.7725892066955566\n",
      "Epoch 2, Loss: 2.7725892066955566\n",
      "Epoch 2, Loss: 2.7725892066955566\n",
      "Epoch 2, Loss: 2.7725892066955566\n",
      "Epoch 2, Loss: 2.7725892066955566\n",
      "Epoch 2, Loss: 2.7725892066955566\n",
      "Epoch 2, Loss: 2.7725892066955566\n",
      "Epoch 2, Loss: 2.7725892066955566\n",
      "Epoch 2, Loss: 2.7725892066955566\n",
      "Epoch 2, Loss: 2.7725892066955566\n",
      "Epoch 2, Loss: 2.7725892066955566\n",
      "Epoch 2, Loss: 2.7725892066955566\n",
      "Epoch 2, Loss: 2.7725892066955566\n",
      "Epoch 2, Loss: 2.7725892066955566\n",
      "Epoch 2, Loss: 2.7725892066955566\n",
      "Epoch 2, Loss: 2.7725892066955566\n",
      "Epoch 2, Loss: 2.7725892066955566\n",
      "Epoch 2, Loss: 2.7725892066955566\n",
      "Epoch 2, Loss: 2.7725892066955566\n",
      "Epoch 2, Loss: 2.7725892066955566\n",
      "Epoch 2, Loss: 2.7725892066955566\n",
      "Epoch 2, Loss: 2.7725892066955566\n",
      "Epoch 2, Loss: 2.7725892066955566\n",
      "Epoch 2, Loss: 2.7725892066955566\n",
      "Epoch 2, Loss: 2.7725892066955566\n",
      "Epoch 2, Loss: 2.7725892066955566\n",
      "Epoch 2, Loss: 2.7725892066955566\n",
      "Epoch 2, Loss: 2.7725892066955566\n",
      "Epoch 2, Loss: 2.7725892066955566\n",
      "Epoch 2, Loss: 2.7725892066955566\n",
      "Epoch 2, Loss: 2.7725892066955566\n",
      "Epoch 2, Loss: 2.7725892066955566\n",
      "Epoch 2, Loss: 2.7725892066955566\n",
      "Epoch 2, Loss: 2.7725892066955566\n",
      "Epoch 2, Loss: 2.7725892066955566\n",
      "Epoch 2, Loss: 1.3862943649291992\n",
      "Epoch 3, Loss: 2.7725892066955566\n",
      "Epoch 3, Loss: 2.7725892066955566\n",
      "Epoch 3, Loss: 2.7725892066955566\n",
      "Epoch 3, Loss: 2.7725892066955566\n",
      "Epoch 3, Loss: 2.7725892066955566\n",
      "Epoch 3, Loss: 2.7725892066955566\n",
      "Epoch 3, Loss: 2.7725892066955566\n",
      "Epoch 3, Loss: 2.7725892066955566\n",
      "Epoch 3, Loss: 2.7725892066955566\n",
      "Epoch 3, Loss: 2.7725892066955566\n",
      "Epoch 3, Loss: 2.7725892066955566\n",
      "Epoch 3, Loss: 2.7725892066955566\n",
      "Epoch 3, Loss: 2.7725892066955566\n",
      "Epoch 3, Loss: 2.7725892066955566\n",
      "Epoch 3, Loss: 2.7725892066955566\n",
      "Epoch 3, Loss: 2.7725892066955566\n",
      "Epoch 3, Loss: 2.7725892066955566\n",
      "Epoch 3, Loss: 2.7725892066955566\n",
      "Epoch 3, Loss: 2.7725892066955566\n",
      "Epoch 3, Loss: 2.7725892066955566\n",
      "Epoch 3, Loss: 2.7725892066955566\n",
      "Epoch 3, Loss: 2.7725892066955566\n",
      "Epoch 3, Loss: 2.7725892066955566\n",
      "Epoch 3, Loss: 2.7725892066955566\n",
      "Epoch 3, Loss: 2.7725892066955566\n",
      "Epoch 3, Loss: 2.7725892066955566\n",
      "Epoch 3, Loss: 2.7725892066955566\n",
      "Epoch 3, Loss: 2.7725892066955566\n",
      "Epoch 3, Loss: 2.7725892066955566\n",
      "Epoch 3, Loss: 2.7725892066955566\n",
      "Epoch 3, Loss: 2.7725892066955566\n",
      "Epoch 3, Loss: 2.7725892066955566\n",
      "Epoch 3, Loss: 2.7725892066955566\n",
      "Epoch 3, Loss: 2.7725892066955566\n",
      "Epoch 3, Loss: 2.7725892066955566\n",
      "Epoch 3, Loss: 2.7725892066955566\n",
      "Epoch 3, Loss: 2.7725892066955566\n",
      "Epoch 3, Loss: 2.7725892066955566\n",
      "Epoch 3, Loss: 2.7725892066955566\n",
      "Epoch 3, Loss: 2.7725892066955566\n",
      "Epoch 3, Loss: 2.7725892066955566\n",
      "Epoch 3, Loss: 2.7725892066955566\n",
      "Epoch 3, Loss: 2.7725892066955566\n",
      "Epoch 3, Loss: 2.7725892066955566\n",
      "Epoch 3, Loss: 2.7725892066955566\n",
      "Epoch 3, Loss: 2.7725892066955566\n",
      "Epoch 3, Loss: 2.7725892066955566\n",
      "Epoch 3, Loss: 2.7725892066955566\n",
      "Epoch 3, Loss: 2.7725892066955566\n",
      "Epoch 3, Loss: 2.7725892066955566\n",
      "Epoch 3, Loss: 2.7725892066955566\n",
      "Epoch 3, Loss: 1.3862943649291992\n",
      "Epoch 4, Loss: 2.7725892066955566\n",
      "Epoch 4, Loss: 2.7725892066955566\n",
      "Epoch 4, Loss: 2.7725892066955566\n",
      "Epoch 4, Loss: 2.7725892066955566\n",
      "Epoch 4, Loss: 2.7725892066955566\n",
      "Epoch 4, Loss: 2.7725892066955566\n",
      "Epoch 4, Loss: 2.7725892066955566\n",
      "Epoch 4, Loss: 2.7725892066955566\n",
      "Epoch 4, Loss: 2.7725892066955566\n",
      "Epoch 4, Loss: 2.7725892066955566\n",
      "Epoch 4, Loss: 2.7725892066955566\n",
      "Epoch 4, Loss: 2.7725892066955566\n",
      "Epoch 4, Loss: 2.7725892066955566\n",
      "Epoch 4, Loss: 2.7725892066955566\n",
      "Epoch 4, Loss: 2.7725892066955566\n",
      "Epoch 4, Loss: 2.7725892066955566\n",
      "Epoch 4, Loss: 2.7725892066955566\n",
      "Epoch 4, Loss: 2.7725892066955566\n",
      "Epoch 4, Loss: 2.7725892066955566\n",
      "Epoch 4, Loss: 2.7725892066955566\n",
      "Epoch 4, Loss: 2.7725892066955566\n",
      "Epoch 4, Loss: 2.7725892066955566\n",
      "Epoch 4, Loss: 2.7725892066955566\n",
      "Epoch 4, Loss: 2.7725892066955566\n",
      "Epoch 4, Loss: 2.7725892066955566\n",
      "Epoch 4, Loss: 2.7725892066955566\n",
      "Epoch 4, Loss: 2.7725892066955566\n",
      "Epoch 4, Loss: 2.7725892066955566\n",
      "Epoch 4, Loss: 2.7725892066955566\n",
      "Epoch 4, Loss: 2.7725892066955566\n",
      "Epoch 4, Loss: 2.7725892066955566\n",
      "Epoch 4, Loss: 2.7725892066955566\n",
      "Epoch 4, Loss: 2.7725892066955566\n",
      "Epoch 4, Loss: 2.7725892066955566\n",
      "Epoch 4, Loss: 2.7725892066955566\n",
      "Epoch 4, Loss: 2.7725892066955566\n",
      "Epoch 4, Loss: 2.7725892066955566\n",
      "Epoch 4, Loss: 2.7725892066955566\n",
      "Epoch 4, Loss: 2.7725892066955566\n",
      "Epoch 4, Loss: 2.7725892066955566\n",
      "Epoch 4, Loss: 2.7725892066955566\n",
      "Epoch 4, Loss: 2.7725892066955566\n",
      "Epoch 4, Loss: 2.7725892066955566\n",
      "Epoch 4, Loss: 2.7725892066955566\n",
      "Epoch 4, Loss: 2.7725892066955566\n",
      "Epoch 4, Loss: 2.7725892066955566\n",
      "Epoch 4, Loss: 2.7725892066955566\n",
      "Epoch 4, Loss: 2.7725892066955566\n",
      "Epoch 4, Loss: 2.7725892066955566\n",
      "Epoch 4, Loss: 2.7725892066955566\n",
      "Epoch 4, Loss: 2.7725892066955566\n",
      "Epoch 4, Loss: 1.3862943649291992\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# 1. Load the CSV containing filenames and labels\n",
    "df = pd.read_csv(r\"Thales Labelled.csv\")  # Adjust CSV path\n",
    "\n",
    "# 2. Define the image folder path\n",
    "image_folder = r\"Thales Images\"  # Provide path to your image folder\n",
    "\n",
    "# 3. Load Pretrained CLIP Model and Processor\n",
    "model = CLIPModel.from_pretrained('openai/clip-vit-base-patch32')\n",
    "processor = CLIPProcessor.from_pretrained('openai/clip-vit-base-patch32')\n",
    "\n",
    "# 4. Define the label mapping (offensive -> 1, non-offensive -> 0)\n",
    "label_mapping = {'NON-OFFENSIVE': 0, 'OFFENSIVE': 1}\n",
    "\n",
    "# 5. Custom Dataset for your images\n",
    "class DoodleDataset(Dataset):\n",
    "    def __init__(self, dataframe, processor, image_folder, transform=None):\n",
    "        self.dataframe = dataframe\n",
    "        self.processor = processor\n",
    "        self.image_folder = image_folder\n",
    "        self.transform = transform\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # Load the image\n",
    "        filename = self.dataframe.iloc[idx]['filename']  # Get filename from CSV\n",
    "        image_path = os.path.join(self.image_folder, filename)  # Construct full image path\n",
    "        image = Image.open(image_path).convert(\"RGB\")\n",
    "        \n",
    "        # Text label for reasoning\n",
    "        label = self.dataframe.iloc[idx]['label']  # 'offensive' or 'non-offensive'\n",
    "        reasoning = \"This doodle is offensive because it depicts harmful content.\" if label == 'offensive' else \"This doodle is non-offensive because it depicts harmless content.\"\n",
    "        \n",
    "        # Preprocess the image and text separately\n",
    "        inputs = self.processor(text=reasoning, images=image, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "        \n",
    "        # Extract the image tensor and text input ids separately\n",
    "        pixel_values = inputs[\"pixel_values\"].squeeze(0)  # Image tensor (remove batch dimension)\n",
    "        input_ids = inputs[\"input_ids\"].squeeze(0)  # Text input IDs\n",
    "        \n",
    "        # Map the label to an integer (0 or 1)\n",
    "        label = label_mapping[label]\n",
    "        \n",
    "        return pixel_values, input_ids, label\n",
    "\n",
    "# 6. Custom collate function to handle different image sizes\n",
    "def collate_fn(batch):\n",
    "    pixel_values = [item[0] for item in batch]\n",
    "    input_ids = [item[1] for item in batch]\n",
    "    labels = [item[2] for item in batch]\n",
    "    \n",
    "    # Stack the pixel values and input ids to create a batch of consistent size\n",
    "    pixel_values = torch.stack(pixel_values, dim=0)\n",
    "    input_ids = torch.stack(input_ids, dim=0)\n",
    "    labels = torch.tensor(labels, dtype=torch.long)\n",
    "    \n",
    "    return pixel_values, input_ids, labels\n",
    "\n",
    "# 7. Prepare DataLoader with custom collate_fn\n",
    "train_dataset = DoodleDataset(df, processor, image_folder)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=16, shuffle=True, collate_fn=collate_fn)\n",
    "\n",
    "# 8. Set optimizer and loss function\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-5)\n",
    "\n",
    "# 9. Fine-tune CLIP\n",
    "model.train()\n",
    "for epoch in range(5):  # You can adjust the number of epochs\n",
    "    for batch in train_dataloader:\n",
    "        pixel_values, input_ids, labels = batch\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(input_ids=input_ids, pixel_values=pixel_values)\n",
    "        logits_per_image = outputs.logits_per_image  # Image-text similarity logits\n",
    "        \n",
    "        # Define loss (cross-entropy for classification)\n",
    "        loss = torch.nn.CrossEntropyLoss()(logits_per_image, labels)\n",
    "        \n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        print(f\"Epoch {epoch}, Loss: {loss.item()}\")\n",
    "\n",
    "# Ensure all model parameters are contiguous\n",
    "for param in model.parameters():\n",
    "    param.data = param.data.contiguous()\n",
    "\n",
    "\n",
    "# 10. Save the fine-tuned model\n",
    "model.save_pretrained('fine_tuned_clip_model')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4eb59f1a-6825-4ec3-a919-a7cc20a7e6ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted label for the test image: NON-OFFENSIVE\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "from PIL import Image\n",
    "\n",
    "# 1. Load the fine-tuned CLIP model and processor\n",
    "model = CLIPModel.from_pretrained('fine_tuned_clip_model')\n",
    "processor = CLIPProcessor.from_pretrained('openai/clip-vit-base-patch32')\n",
    "\n",
    "# 2. Define label mapping (same as during training)\n",
    "label_mapping = {0: 'NON-OFFENSIVE', 1: 'OFFENSIVE'}\n",
    "\n",
    "# 3. Load and preprocess your test image\n",
    "def test_image(image_path, reasoning_text):\n",
    "    # Load and preprocess the image\n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "    \n",
    "    # Process the image and reasoning text\n",
    "    inputs = processor(text=reasoning_text, images=image, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "    pixel_values = inputs[\"pixel_values\"]  # Image tensor\n",
    "    input_ids = inputs[\"input_ids\"]  # Text input IDs\n",
    "    \n",
    "    return pixel_values, input_ids\n",
    "\n",
    "# 4. Function to predict\n",
    "def predict(image_path):\n",
    "    # Define reasoning text\n",
    "    reasoning_text = \"This doodle is offensive because it depicts harmful content or symbols.\" \\\n",
    "                     \" Otherwise, it is considered non-offensive because it depicts benign content.\"\n",
    "    \n",
    "    # Preprocess the image and text\n",
    "    pixel_values, input_ids = test_image(image_path, reasoning_text)\n",
    "    \n",
    "    # Ensure model is in evaluation mode\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        # Forward pass\n",
    "        outputs = model(input_ids=input_ids, pixel_values=pixel_values)\n",
    "        logits_per_image = outputs.logits_per_image  # Image-text similarity logits\n",
    "        \n",
    "        # Get the predicted class\n",
    "        prediction = torch.argmax(logits_per_image, dim=1).item()\n",
    "        label = label_mapping[prediction]\n",
    "    \n",
    "    return label\n",
    "\n",
    "# 5. Test the model with your image\n",
    "image_path = r\"C:\\Users\\Nitya\\Downloads\\WhatsApp Image 2024-12-06 at 23.34.50_b2745a14.jpg\"  # Replace with your image's path\n",
    "predicted_label = predict(image_path)\n",
    "print(f\"Predicted label for the test image: {predicted_label}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9811ed44-539f-4621-98c3-8796b78af24e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
